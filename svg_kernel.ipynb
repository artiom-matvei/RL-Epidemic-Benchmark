{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD = 7\n",
    "\n",
    "# Env\n",
    "import gym, json\n",
    "from gym import spaces\n",
    "from epipolicy.core.epidemic import construct_epidemic\n",
    "from epipolicy.obj.act import construct_act\n",
    "\n",
    "class EpiEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, session):\n",
    "        super(EpiEnv, self).__init__()\n",
    "        self.epi = construct_epidemic(session)\n",
    "        total_population = np.sum(self.epi.static.default_state.obs.current_comp)\n",
    "        obs_count = self.epi.static.compartment_count * self.epi.static.locale_count * self.epi.static.group_count\n",
    "        action_count = 0\n",
    "        action_param_count =  0\n",
    "        for itv in self.epi.static.interventions:\n",
    "            if not itv.is_cost:\n",
    "                action_count += 1\n",
    "                action_param_count += len(itv.cp_list)\n",
    "        self.act_domain = np.zeros((action_param_count, 2), dtype=np.float64)\n",
    "        index = 0\n",
    "        for itv in self.epi.static.interventions:\n",
    "            if not itv.is_cost:\n",
    "                for cp in itv.cp_list:\n",
    "                    self.act_domain[index, 0] = cp.min_value\n",
    "                    self.act_domain[index, 1] = cp.max_value\n",
    "                    index += 1\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(action_count,), dtype=np.float64)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=total_population, shape=(obs_count,), dtype=np.float64)\n",
    "\n",
    "    def step(self, action):\n",
    "        expanded_action = np.zeros(len(self.act_domain), dtype=np.float64)\n",
    "        index = 0\n",
    "        for i in range(len(self.act_domain)):\n",
    "            if self.act_domain[i, 0] == self.act_domain[i, 1]:\n",
    "                expanded_action[i] = self.act_domain[i, 0]\n",
    "            else:\n",
    "                expanded_action[i] = action[index]\n",
    "                index += 1\n",
    "\n",
    "        epi_action = []\n",
    "        index = 0\n",
    "        for itv_id, itv in enumerate(self.epi.static.interventions):\n",
    "            if not itv.is_cost:\n",
    "                epi_action.append(construct_act(itv_id, expanded_action[index:index+len(itv.cp_list)]))\n",
    "                index += len(itv.cp_list)\n",
    "\n",
    "        total_r = 0\n",
    "        for i in range(PERIOD):\n",
    "            state, r, done = self.epi.step(epi_action)\n",
    "            total_r += r\n",
    "            if done:\n",
    "                break\n",
    "        return state.obs.current_comp.flatten(), total_r, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.epi.reset()\n",
    "        return state.obs.current_comp.flatten()  # reward, done, info can't be included\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(main_args = None):\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--exp-name\", type=str, default=\"PPO\",\n",
    "        help=\"the name of this experiment\")\n",
    "    parser.add_argument(\"--gym-id\", type=str, default=\"HalfCheetahBulletEnv-v0\",\n",
    "        help=\"the id of the gym environment\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=3e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=700000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"ppo-implementation-details\",\n",
    "        help=\"the wandb's project name\")\n",
    "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "        help=\"the entity (team) of wandb's project\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
    "\n",
    "    parser.add_argument(\"--policy_plot_interval\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    parser.add_argument(\"--num-envs\", type=int, default=1,\n",
    "        help=\"the number of parallel game environments\")\n",
    "    parser.add_argument(\"--num-steps\", type=int, default=2048,\n",
    "        help=\"the number of steps to run in each environment per policy rollout\")\n",
    "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
    "    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Use GAE for advantage computation\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
    "        help=\"the lambda for the general advantage estimation\")\n",
    "    parser.add_argument(\"--num-minibatches\", type=int, default=32,\n",
    "        help=\"the number of mini-batches\")\n",
    "    parser.add_argument(\"--update-epochs\", type=int, default=10,\n",
    "        help=\"the K epochs to update the policy\")\n",
    "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles advantages normalization\")\n",
    "    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n",
    "        help=\"the surrogate clipping coefficient\")\n",
    "    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.0,\n",
    "        help=\"coefficient of the entropy\")\n",
    "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
    "        help=\"coefficient of the value function\")\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
    "        help=\"the maximum norm for the gradient clipping\")\n",
    "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
    "        help=\"the target KL divergence threshold\")\n",
    "    if main_args is not None:\n",
    "        args = parser.parse_args(main_args.split())\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    args.num_steps //= PERIOD\n",
    "    args.total_timesteps //= PERIOD\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_ids = [\"SIR_A\"]#, \"SIR_B\", \"SIRV_A\", \"SIRV_B\", \"COVID_A\", \"COVID_B\", \"COVID_C\"]\n",
    "\n",
    "def make_env(gym_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if gym_id in epi_ids:\n",
    "            fp = open('jsons/{}.json'.format(gym_id), 'r')\n",
    "            session = json.load(fp)\n",
    "            env = EpiEnv(session)\n",
    "        else:\n",
    "            env = gym.make(gym_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        # Our env is deterministic\n",
    "        # env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def make_primal_env(gym_id):\n",
    "    def thunk():\n",
    "        if gym_id in epi_ids:\n",
    "            fp = open('jsons/{}.json'.format(gym_id), 'r')\n",
    "            session = json.load(fp)\n",
    "            env = EpiEnv(session)\n",
    "        else:\n",
    "            env = gym.make(gym_id)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.actor_mean_sigma = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 2*np.prod(envs.single_action_space.shape)), std=0.01),\n",
    "        )\n",
    "        self.m = MultivariateNormal(\n",
    "            torch.zeros(np.prod(envs.single_action_space.shape)), \n",
    "            torch.eye(np.prod(envs.single_action_space.shape))\n",
    "        )\n",
    "\n",
    "    def get_action(self, x):\n",
    "        action_mean = self.actor_mean_sigma(x)[:np.prod(envs.single_action_space.shape)]\n",
    "        action_sigma = self.actor_mean_sigma(x)[np.prod(envs.single_action_space.shape):]\n",
    "        epsilon = m.sample()\n",
    "        \n",
    "        action = action_mean + action_sigma * epsilon \n",
    "        # TODO: ensure action is within boundaries\n",
    "        return action, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SIR_A__PPO__1__1734057822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:3: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from int64 to int32. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "<string>:3: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from float64 to float32. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "<string>:3: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from int64 to float32. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  0%|          | 0/342 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=53, episodic_return=-133795024.0\n",
      "global_step=106, episodic_return=-249479552.0\n",
      "global_step=159, episodic_return=-330727008.0\n",
      "global_step=212, episodic_return=-109690464.0\n",
      "global_step=265, episodic_return=-127306376.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/342 [00:23<2:12:57, 23.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 292, total_rewards=-2099613583.1634524\n",
      "global_step=345, episodic_return=-146012256.0\n",
      "global_step=398, episodic_return=-339601056.0\n",
      "global_step=451, episodic_return=-665188416.0\n",
      "global_step=504, episodic_return=-726868480.0\n",
      "global_step=557, episodic_return=-147837248.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/342 [00:46<2:11:52, 23.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 584, total_rewards=-2040265843.0619414\n",
      "global_step=637, episodic_return=-307461856.0\n",
      "global_step=690, episodic_return=-99795864.0\n",
      "global_step=743, episodic_return=-158665840.0\n",
      "global_step=796, episodic_return=-114541504.0\n",
      "global_step=849, episodic_return=-901531840.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/342 [01:09<2:09:33, 22.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 876, total_rewards=-1734867601.4887385\n",
      "global_step=929, episodic_return=-445313216.0\n",
      "global_step=982, episodic_return=-83600248.0\n",
      "global_step=1035, episodic_return=-204367056.0\n",
      "global_step=1088, episodic_return=-86524936.0\n",
      "global_step=1141, episodic_return=-98369328.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/342 [01:31<2:08:22, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 1168, total_rewards=-74881429.28981504\n",
      "global_step=1221, episodic_return=-178455440.0\n",
      "global_step=1274, episodic_return=-87397936.0\n",
      "global_step=1327, episodic_return=-171950560.0\n",
      "global_step=1380, episodic_return=-104134096.0\n",
      "global_step=1433, episodic_return=-83350528.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/342 [01:54<2:07:55, 22.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 1460, total_rewards=-77379707.01438811\n",
      "global_step=1513, episodic_return=-84824904.0\n",
      "global_step=1566, episodic_return=-88166232.0\n",
      "global_step=1619, episodic_return=-94956096.0\n",
      "global_step=1672, episodic_return=-102244672.0\n",
      "global_step=1725, episodic_return=-91697304.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/342 [02:18<2:10:36, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 1752, total_rewards=-79578263.65854432\n",
      "global_step=1805, episodic_return=-116219592.0\n",
      "global_step=1858, episodic_return=-95018544.0\n",
      "global_step=1911, episodic_return=-87980648.0\n",
      "global_step=1964, episodic_return=-88118280.0\n",
      "global_step=2017, episodic_return=-84181952.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/342 [02:42<2:11:46, 23.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 2044, total_rewards=-76738271.50424446\n",
      "global_step=2097, episodic_return=-84659872.0\n",
      "global_step=2150, episodic_return=-91678368.0\n",
      "global_step=2203, episodic_return=-95625480.0\n",
      "global_step=2256, episodic_return=-90566856.0\n",
      "global_step=2309, episodic_return=-89653352.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/342 [03:06<2:11:41, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 2336, total_rewards=-75981280.01219437\n",
      "global_step=2389, episodic_return=-81829624.0\n",
      "global_step=2442, episodic_return=-77562032.0\n",
      "global_step=2495, episodic_return=-88878536.0\n",
      "global_step=2548, episodic_return=-79239176.0\n",
      "global_step=2601, episodic_return=-84469976.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9/342 [03:29<2:10:07, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 2628, total_rewards=-70506108.15418589\n",
      "global_step=2681, episodic_return=-77444712.0\n",
      "global_step=2734, episodic_return=-87007728.0\n",
      "global_step=2787, episodic_return=-86875888.0\n",
      "global_step=2840, episodic_return=-77934544.0\n",
      "global_step=2893, episodic_return=-74851952.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/342 [03:52<2:09:16, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 2920, total_rewards=-67381805.80293567\n",
      "global_step=2973, episodic_return=-78005424.0\n",
      "global_step=3026, episodic_return=-79989776.0\n",
      "global_step=3079, episodic_return=-81408256.0\n",
      "global_step=3132, episodic_return=-72861464.0\n",
      "global_step=3185, episodic_return=-74645584.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 11/342 [04:15<2:07:19, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 3212, total_rewards=-64335856.063965864\n",
      "global_step=3265, episodic_return=-67815864.0\n",
      "global_step=3318, episodic_return=-72752288.0\n",
      "global_step=3371, episodic_return=-71911888.0\n",
      "global_step=3424, episodic_return=-71556880.0\n",
      "global_step=3477, episodic_return=-81386176.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 12/342 [04:37<2:05:49, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 3504, total_rewards=-62402387.85231479\n",
      "global_step=3557, episodic_return=-76875008.0\n",
      "global_step=3610, episodic_return=-72770112.0\n",
      "global_step=3663, episodic_return=-68624648.0\n",
      "global_step=3716, episodic_return=-70534200.0\n",
      "global_step=3769, episodic_return=-70005352.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 13/342 [05:01<2:06:34, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 3796, total_rewards=-60686762.79950226\n",
      "global_step=3849, episodic_return=-72333864.0\n",
      "global_step=3902, episodic_return=-67380064.0\n",
      "global_step=3955, episodic_return=-67931592.0\n",
      "global_step=4008, episodic_return=-84924960.0\n",
      "global_step=4061, episodic_return=-71303152.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 14/342 [05:23<2:04:59, 22.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 4088, total_rewards=-62711694.538359955\n",
      "global_step=4141, episodic_return=-73808248.0\n",
      "global_step=4194, episodic_return=-76212280.0\n",
      "global_step=4247, episodic_return=-70205912.0\n",
      "global_step=4300, episodic_return=-67460720.0\n",
      "global_step=4353, episodic_return=-70003136.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 15/342 [05:46<2:04:01, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 4380, total_rewards=-58569773.920522265\n",
      "global_step=4433, episodic_return=-72629384.0\n",
      "global_step=4486, episodic_return=-64818832.0\n",
      "global_step=4539, episodic_return=-70458664.0\n",
      "global_step=4592, episodic_return=-64300480.0\n",
      "global_step=4645, episodic_return=-72019528.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 16/342 [06:09<2:04:08, 22.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 4672, total_rewards=-59394951.08730896\n",
      "global_step=4725, episodic_return=-68378952.0\n",
      "global_step=4778, episodic_return=-64129992.0\n",
      "global_step=4831, episodic_return=-64226284.0\n",
      "global_step=4884, episodic_return=-70788824.0\n",
      "global_step=4937, episodic_return=-63923412.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 17/342 [06:31<2:03:09, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 4964, total_rewards=-61656280.554615654\n",
      "global_step=5017, episodic_return=-68673680.0\n",
      "global_step=5070, episodic_return=-63926916.0\n",
      "global_step=5123, episodic_return=-66350820.0\n",
      "global_step=5176, episodic_return=-66612476.0\n",
      "global_step=5229, episodic_return=-68279512.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 18/342 [06:55<2:03:48, 22.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 5256, total_rewards=-57133070.249809675\n",
      "global_step=5309, episodic_return=-74974008.0\n",
      "global_step=5362, episodic_return=-63797024.0\n",
      "global_step=5415, episodic_return=-64817332.0\n",
      "global_step=5468, episodic_return=-61595000.0\n",
      "global_step=5521, episodic_return=-63766048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 19/342 [07:17<2:02:53, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 5548, total_rewards=-56398052.46691203\n",
      "global_step=5601, episodic_return=-62686956.0\n",
      "global_step=5654, episodic_return=-62832724.0\n",
      "global_step=5707, episodic_return=-61992280.0\n",
      "global_step=5760, episodic_return=-58220200.0\n",
      "global_step=5813, episodic_return=-68500144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 20/342 [07:41<2:04:44, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 5840, total_rewards=-52527271.392345324\n",
      "global_step=5893, episodic_return=-57496456.0\n",
      "global_step=5946, episodic_return=-63416596.0\n",
      "global_step=5999, episodic_return=-60338556.0\n",
      "global_step=6052, episodic_return=-63159872.0\n",
      "global_step=6105, episodic_return=-63926980.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 21/342 [08:04<2:03:21, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 6132, total_rewards=-50772770.67273957\n",
      "global_step=6185, episodic_return=-65014184.0\n",
      "global_step=6238, episodic_return=-60727692.0\n",
      "global_step=6291, episodic_return=-57287460.0\n",
      "global_step=6344, episodic_return=-63917208.0\n",
      "global_step=6397, episodic_return=-63224068.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 22/342 [08:26<2:01:54, 22.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 6424, total_rewards=-46126588.3365019\n",
      "global_step=6477, episodic_return=-58712124.0\n",
      "global_step=6530, episodic_return=-56496916.0\n",
      "global_step=6583, episodic_return=-55983256.0\n",
      "global_step=6636, episodic_return=-60844784.0\n",
      "global_step=6689, episodic_return=-61305432.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 23/342 [08:49<2:00:43, 22.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 6716, total_rewards=-43624394.94834028\n",
      "global_step=6769, episodic_return=-57083516.0\n",
      "global_step=6822, episodic_return=-59604772.0\n",
      "global_step=6875, episodic_return=-62715324.0\n",
      "global_step=6928, episodic_return=-55597404.0\n",
      "global_step=6981, episodic_return=-56456508.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 24/342 [09:12<2:01:44, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 7008, total_rewards=-43891715.69008464\n",
      "global_step=7061, episodic_return=-55910000.0\n",
      "global_step=7114, episodic_return=-63495888.0\n",
      "global_step=7167, episodic_return=-60106960.0\n",
      "global_step=7220, episodic_return=-54380316.0\n",
      "global_step=7273, episodic_return=-55668144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 25/342 [09:35<2:01:03, 22.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 7300, total_rewards=-44049822.720060065\n",
      "global_step=7353, episodic_return=-55698352.0\n",
      "global_step=7406, episodic_return=-55994464.0\n",
      "global_step=7459, episodic_return=-49088484.0\n",
      "global_step=7512, episodic_return=-53138228.0\n",
      "global_step=7565, episodic_return=-56414444.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 26/342 [09:58<2:00:29, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 7592, total_rewards=-46584822.970904656\n",
      "global_step=7645, episodic_return=-57426632.0\n",
      "global_step=7698, episodic_return=-55533768.0\n",
      "global_step=7751, episodic_return=-59041956.0\n",
      "global_step=7804, episodic_return=-56720716.0\n",
      "global_step=7857, episodic_return=-54987312.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 27/342 [10:22<2:01:53, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 7884, total_rewards=-48816291.19711381\n",
      "global_step=7937, episodic_return=-55528116.0\n",
      "global_step=7990, episodic_return=-54061548.0\n",
      "global_step=8043, episodic_return=-53545924.0\n",
      "global_step=8096, episodic_return=-53677144.0\n",
      "global_step=8149, episodic_return=-53739072.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 28/342 [10:44<1:59:53, 22.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 8176, total_rewards=-45585212.819101445\n",
      "global_step=8229, episodic_return=-54453132.0\n",
      "global_step=8282, episodic_return=-52782240.0\n",
      "global_step=8335, episodic_return=-53756364.0\n",
      "global_step=8388, episodic_return=-52698432.0\n",
      "global_step=8441, episodic_return=-52214416.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 29/342 [11:07<1:58:39, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 8468, total_rewards=-44471375.82020828\n",
      "global_step=8521, episodic_return=-57264964.0\n",
      "global_step=8574, episodic_return=-55219548.0\n",
      "global_step=8627, episodic_return=-54101740.0\n",
      "global_step=8680, episodic_return=-52382356.0\n",
      "global_step=8733, episodic_return=-52053896.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 30/342 [11:30<1:58:43, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 8760, total_rewards=-45806104.54661074\n",
      "global_step=8813, episodic_return=-57861564.0\n",
      "global_step=8866, episodic_return=-52430804.0\n",
      "global_step=8919, episodic_return=-51889528.0\n",
      "global_step=8972, episodic_return=-51273380.0\n",
      "global_step=9025, episodic_return=-52959388.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 31/342 [11:52<1:58:14, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 9052, total_rewards=-45439997.79261839\n",
      "global_step=9105, episodic_return=-53672604.0\n",
      "global_step=9158, episodic_return=-53851952.0\n",
      "global_step=9211, episodic_return=-54898312.0\n",
      "global_step=9264, episodic_return=-52499888.0\n",
      "global_step=9317, episodic_return=-51427676.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 32/342 [12:15<1:57:15, 22.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 9344, total_rewards=-49086586.62170096\n",
      "global_step=9397, episodic_return=-54347072.0\n",
      "global_step=9450, episodic_return=-55274460.0\n",
      "global_step=9503, episodic_return=-50691476.0\n",
      "global_step=9556, episodic_return=-49646076.0\n",
      "global_step=9609, episodic_return=-56541536.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 33/342 [12:42<2:04:25, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 9636, total_rewards=-53855021.737694204\n",
      "global_step=9689, episodic_return=-54022264.0\n",
      "global_step=9742, episodic_return=-50207368.0\n",
      "global_step=9795, episodic_return=-53143744.0\n",
      "global_step=9848, episodic_return=-54503508.0\n",
      "global_step=9901, episodic_return=-49656420.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 34/342 [13:10<2:09:24, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 9928, total_rewards=-53137225.77185479\n",
      "global_step=9981, episodic_return=-52339012.0\n",
      "global_step=10034, episodic_return=-49936476.0\n",
      "global_step=10087, episodic_return=-53060108.0\n",
      "global_step=10140, episodic_return=-46798888.0\n",
      "global_step=10193, episodic_return=-56195652.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 35/342 [13:44<2:22:28, 27.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 10220, total_rewards=-53185416.54949258\n",
      "global_step=10273, episodic_return=-48516876.0\n",
      "global_step=10326, episodic_return=-48999888.0\n",
      "global_step=10379, episodic_return=-56298220.0\n",
      "global_step=10432, episodic_return=-52533684.0\n",
      "global_step=10485, episodic_return=-53379628.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 36/342 [14:09<2:18:05, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 10512, total_rewards=-47100381.44439803\n",
      "global_step=10565, episodic_return=-54314132.0\n",
      "global_step=10618, episodic_return=-47492928.0\n",
      "global_step=10671, episodic_return=-50886320.0\n",
      "global_step=10724, episodic_return=-47876848.0\n",
      "global_step=10777, episodic_return=-48905232.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 37/342 [14:36<2:16:25, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 10804, total_rewards=-48007064.55710171\n",
      "global_step=10857, episodic_return=-50189580.0\n",
      "global_step=10910, episodic_return=-47586332.0\n",
      "global_step=10963, episodic_return=-49825056.0\n",
      "global_step=11016, episodic_return=-50066172.0\n",
      "global_step=11069, episodic_return=-48960016.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 38/342 [15:06<2:21:45, 27.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 11096, total_rewards=-46363218.65324462\n",
      "global_step=11149, episodic_return=-59491360.0\n",
      "global_step=11202, episodic_return=-62188644.0\n",
      "global_step=11255, episodic_return=-46890900.0\n",
      "global_step=11308, episodic_return=-48425652.0\n",
      "global_step=11361, episodic_return=-47930188.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 39/342 [15:36<2:24:38, 28.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 11388, total_rewards=-46226250.25802873\n",
      "global_step=11441, episodic_return=-52877752.0\n",
      "global_step=11494, episodic_return=-49557028.0\n",
      "global_step=11547, episodic_return=-47128444.0\n",
      "global_step=11600, episodic_return=-48455184.0\n",
      "global_step=11653, episodic_return=-51297872.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 40/342 [16:04<2:21:58, 28.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 11680, total_rewards=-45879422.072195515\n",
      "global_step=11733, episodic_return=-56921572.0\n",
      "global_step=11786, episodic_return=-50882424.0\n",
      "global_step=11839, episodic_return=-49350592.0\n",
      "global_step=11892, episodic_return=-47142480.0\n",
      "global_step=11945, episodic_return=-47647960.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 41/342 [16:30<2:18:47, 27.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 11972, total_rewards=-44715964.006441206\n",
      "global_step=12025, episodic_return=-60272560.0\n",
      "global_step=12078, episodic_return=-45574256.0\n",
      "global_step=12131, episodic_return=-49165588.0\n",
      "global_step=12184, episodic_return=-47125136.0\n",
      "global_step=12237, episodic_return=-52258168.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 42/342 [16:56<2:16:01, 27.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 12264, total_rewards=-44206595.9860551\n",
      "global_step=12317, episodic_return=-51284428.0\n",
      "global_step=12370, episodic_return=-49159856.0\n",
      "global_step=12423, episodic_return=-46140484.0\n",
      "global_step=12476, episodic_return=-49758736.0\n",
      "global_step=12529, episodic_return=-47168432.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 43/342 [17:21<2:11:40, 26.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 12556, total_rewards=-43655797.32124931\n",
      "global_step=12609, episodic_return=-49859296.0\n",
      "global_step=12662, episodic_return=-47713124.0\n",
      "global_step=12715, episodic_return=-46323676.0\n",
      "global_step=12768, episodic_return=-47953068.0\n",
      "global_step=12821, episodic_return=-47020724.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 44/342 [17:47<2:10:19, 26.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 12848, total_rewards=-43914766.042894915\n",
      "global_step=12901, episodic_return=-51268144.0\n",
      "global_step=12954, episodic_return=-50072824.0\n",
      "global_step=13007, episodic_return=-48291044.0\n",
      "global_step=13060, episodic_return=-50581336.0\n",
      "global_step=13113, episodic_return=-47913820.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 45/342 [18:13<2:09:58, 26.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At global step 13140, total_rewards=-43549887.85791444\n",
      "global_step=13193, episodic_return=-53323212.0\n",
      "global_step=13246, episodic_return=-53984352.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 45/342 [18:21<2:01:09, 24.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# TRY NOT TO MODIFY: execute the game and log data.\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     72\u001b[0m next_obs, next_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(next_obs)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mTensor(done)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\vector\\vector_env.py:94\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each sub-environments.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    A list of auxiliary diagnostic information dicts from sub-environments.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:83\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[1;32m---> 83\u001b[0m     observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones[i], info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones[i]:\n\u001b[0;32m     85\u001b[0m         observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\core.py:336\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 336\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), done, info\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\wrappers\\normalize.py:93\u001b[0m, in \u001b[0;36mNormalizeReward.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 93\u001b[0m     obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env:\n\u001b[0;32m     95\u001b[0m         rews \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([rews])\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\wrappers\\normalize.py:57\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 57\u001b[0m     obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env:\n\u001b[0;32m     59\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(obs)\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\core.py:349\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\wrappers\\record_episode_statistics.py:26\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 26\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRecordEpisodeStatistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\gym\\core.py:289\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mEpiEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     56\u001b[0m total_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(PERIOD):\n\u001b[1;32m---> 58\u001b[0m     state, r, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepi_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     total_r \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\epipolicy\\core\\epidemic.py:111\u001b[0m, in \u001b[0;36mEpidemic.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    109\u001b[0m current_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_state()\n\u001b[0;32m    110\u001b[0m normalized_action \u001b[38;5;241m=\u001b[39m get_normalized_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic, action)\n\u001b[1;32m--> 111\u001b[0m next_state, delta_parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(next_state)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mdelta_parameters\u001b[38;5;241m.\u001b[39mappend(delta_parameter)\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\epipolicy\\core\\epidemic.py:87\u001b[0m, in \u001b[0;36mEpidemic.get_next_state\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     84\u001b[0m dest_parameter \u001b[38;5;241m=\u001b[39m get_parameter_from_delta(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic, delta_parameter)\n\u001b[0;32m     85\u001b[0m gap_parameter \u001b[38;5;241m=\u001b[39m get_gap_parameter(state\u001b[38;5;241m.\u001b[39munobs, dest_parameter)\n\u001b[1;32m---> 87\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43msolve_ivp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscipy_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgap_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m next_obs \u001b[38;5;241m=\u001b[39m get_observable_from_flat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic, res\u001b[38;5;241m.\u001b[39my[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m State(state\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, next_obs, dest_parameter), delta_parameter\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py:655\u001b[0m, in \u001b[0;36msolve_ivp\u001b[1;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[0;32m    653\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m solver\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    658\u001b[0m         status \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\base.py:197\u001b[0m, in \u001b[0;36mOdeSolver.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt\n\u001b[1;32m--> 197\u001b[0m     success, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py:144\u001b[0m, in \u001b[0;36mRungeKutta._step_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m h \u001b[38;5;241m=\u001b[39m t_new \u001b[38;5;241m-\u001b[39m t\n\u001b[0;32m    142\u001b[0m h_abs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(h)\n\u001b[1;32m--> 144\u001b[0m y_new, f_new \u001b[38;5;241m=\u001b[39m \u001b[43mrk_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m scale \u001b[38;5;241m=\u001b[39m atol \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(np\u001b[38;5;241m.\u001b[39mabs(y), np\u001b[38;5;241m.\u001b[39mabs(y_new)) \u001b[38;5;241m*\u001b[39m rtol\n\u001b[0;32m    147\u001b[0m error_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_error_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, h, scale)\n",
      "File \u001b[1;32mc:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\.venv\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py:64\u001b[0m, in \u001b[0;36mrk_step\u001b[1;34m(fun, t, y, f, h, A, B, C, K)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, (a, c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(A[\u001b[38;5;241m1\u001b[39m:], C[\u001b[38;5;241m1\u001b[39m:]), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     63\u001b[0m     dy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(K[:s]\u001b[38;5;241m.\u001b[39mT, a[:s]) \u001b[38;5;241m*\u001b[39m h\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mK\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m fun(t \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m*\u001b[39m h, y \u001b[38;5;241m+\u001b[39m dy)\n\u001b[0;32m     66\u001b[0m y_new \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m h \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(K[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT, B)\n\u001b[0;32m     67\u001b[0m f_new \u001b[38;5;241m=\u001b[39m fun(t \u001b[38;5;241m+\u001b[39m h, y_new)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "args = parse_args(\"--gym-id SIR_A --seed 1\")\n",
    "run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "print(\"Running\", run_name)\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    ")\n",
    "# envs = gym.vector.AsyncVectorEnv(\n",
    "#     [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "# )\n",
    "test_env = make_primal_env(args.gym_id)()\n",
    "# test_env = make_env(args.gym_id, args.seed, 0, args.capture_video, run_name)()\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "num_updates = args.total_timesteps // args.batch_size\n",
    "ROLLOUT_HORIZON = 35\n",
    "\n",
    "csv_file = open('runs/{}/records.csv'.format(run_name), 'w')\n",
    "\n",
    "for update in tqdm(range(1, num_updates + 1)):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if args.anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0) / num_updates\n",
    "        lrnow = frac * args.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += 1 * args.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = agent.get_action_and_value(next_obs)\n",
    "        actions[step] = action\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        for item in info:\n",
    "            if \"episode\" in item.keys():\n",
    "                print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                break\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        returns = torch.zeros_like(rewards).to(device)\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "            next_return = returns[t + 1]\n",
    "            returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_returns = returns.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in tqdm(range(max(1, args.update_epochs))): # eventually remove epoch as we look at trajectories only once\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            # TODO: do we actually need mini-batches as we look at all trajectories at once?\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            mb_returns = b_returns[mb_inds]\n",
    "            pg_loss = -mb_returns\n",
    "            \n",
    "            loss = pg_loss \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    # PLOT POLICY\n",
    "    if args.gym_id in epi_ids and (update - 1) % args.policy_plot_interval == 0:\n",
    "        test_obs = torch.Tensor(test_env.reset())\n",
    "        env_obs = torch.Tensor(envs.reset()).to(device)\n",
    "        timestep = 0\n",
    "        total_r = 0\n",
    "        done = False\n",
    "        itv_line = []\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action_mean = agent.get_action_mean(env_obs)\n",
    "                test_action_mean = torch.mean(action_mean, 0)\n",
    "                test_action_mean = torch.clamp(test_action_mean, 0, 1)\n",
    "             \n",
    "            test_obs, r, done, _ = test_env.step(test_action_mean.cpu().numpy())\n",
    "            test_obs = torch.Tensor(test_obs)\n",
    "            itv_index = 0\n",
    "            itv_array = []\n",
    "            for itv in test_env.epi.static.interventions:\n",
    "                if not itv.is_cost:\n",
    "                    v = float(test_action_mean[itv_index])\n",
    "                    writer.add_scalar('charts/policy_{}/{}'.format(global_step, itv.name), v, timestep)\n",
    "                    itv_array.append(v)\n",
    "                    itv_index += 1\n",
    "            itv_line.append(itv_array)\n",
    "            \n",
    "            env_obs, _, _, _ = envs.step(action_mean.cpu().numpy())\n",
    "            env_obs = torch.Tensor(env_obs).to(device)\n",
    "\n",
    "            total_r += r\n",
    "            timestep += PERIOD\n",
    "            \n",
    "        line = '|'.join([str(global_step), str(total_r), str(itv_line)]) + '\\n'\n",
    "        csv_file.write(line)\n",
    "        writer.add_scalar('charts/learning_curve', total_r, global_step)\n",
    "        print(\"At global step {}, total_rewards={}\".format(global_step, total_r))\n",
    "\n",
    "csv_file.close()\n",
    "envs.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
