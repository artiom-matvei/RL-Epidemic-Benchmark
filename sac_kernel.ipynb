{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928119c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PERIOD = 7\n",
    "\n",
    "# Env\n",
    "import gym, json, time\n",
    "import argparse\n",
    "from gym import spaces\n",
    "from epipolicy.core.epidemic import construct_epidemic\n",
    "from epipolicy.obj.act import construct_act\n",
    "import numpy as np\n",
    "\n",
    "class EpiEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, session):\n",
    "        super(EpiEnv, self).__init__()\n",
    "        self.epi = construct_epidemic(session)\n",
    "        total_population = np.sum(self.epi.static.default_state.obs.current_comp)\n",
    "        obs_count = self.epi.static.compartment_count * self.epi.static.locale_count * self.epi.static.group_count\n",
    "        action_count = 0\n",
    "        action_param_count =  0\n",
    "        for itv in self.epi.static.interventions:\n",
    "            if not itv.is_cost:\n",
    "                action_count += 1\n",
    "                action_param_count += len(itv.cp_list)\n",
    "        self.act_domain = np.zeros((action_param_count, 2), dtype=np.float32)\n",
    "        index = 0\n",
    "        for itv in self.epi.static.interventions:\n",
    "            if not itv.is_cost:\n",
    "                for cp in itv.cp_list:\n",
    "                    self.act_domain[index, 0] = cp.min_value\n",
    "                    self.act_domain[index, 1] = cp.max_value\n",
    "                    index += 1\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(action_count,), dtype=np.float32)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=total_population, shape=(obs_count,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        expanded_action = np.zeros(len(self.act_domain), dtype=np.float32)\n",
    "        index = 0\n",
    "        for i in range(len(self.act_domain)):\n",
    "            if self.act_domain[i, 0] == self.act_domain[i, 1]:\n",
    "                expanded_action[i] = self.act_domain[i, 0]\n",
    "            else:\n",
    "                expanded_action[i] = min(max(0, action[index]), 1)\n",
    "                index += 1\n",
    "\n",
    "        epi_action = []\n",
    "        index = 0\n",
    "        for itv_id, itv in enumerate(self.epi.static.interventions):\n",
    "            if not itv.is_cost:\n",
    "                epi_action.append(construct_act(itv_id, expanded_action[index:index+len(itv.cp_list)]))\n",
    "                index += len(itv.cp_list)\n",
    "\n",
    "        total_r = 0\n",
    "        for i in range(PERIOD):\n",
    "            state, r, done = self.epi.step(epi_action)\n",
    "            total_r += r\n",
    "            if done:\n",
    "                break\n",
    "        return state.obs.current_comp.flatten(), total_r, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.epi.reset()\n",
    "        return state.obs.current_comp.flatten()  # reward, done, info can't be included\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class RewardScale(gym.RewardWrapper):\n",
    "    def __init__(self, env, scale):\n",
    "        super().__init__(env)\n",
    "        self.scale = scale\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        # modify rew\n",
    "        return rew * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57301ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epi_ids = [\"SIR_A\", \"SIR_B\", \"SIRV_A\", \"SIRV_B\", \"COVID_A\", \"COVID_B\", \"COVID_C\"]\n",
    "\n",
    "def make_env(gym_id, seed, idx):\n",
    "    def thunk():\n",
    "        if gym_id in epi_ids:\n",
    "            fp = open('jsons/{}.json'.format(gym_id), 'r')\n",
    "            session = json.load(fp)\n",
    "            env = EpiEnv(session)\n",
    "        else:\n",
    "            env = gym.make(gym_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        env = RewardScale(env, 10)\n",
    "        # Our env is deterministic\n",
    "        # env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def make_primal_env(gym_id):\n",
    "    def thunk():\n",
    "        if gym_id in epi_ids:\n",
    "            fp = open('jsons/{}.json'.format(gym_id), 'r')\n",
    "            session = json.load(fp)\n",
    "            env = EpiEnv(session)\n",
    "        else:\n",
    "            env = gym.make(gym_id)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b1dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_args(main_args = None):\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--exp-name\", type=str, default=\"SAC\",\n",
    "        help=\"the name of this experiment\")\n",
    "    parser.add_argument(\"--gym-id\", type=str, default=\"jsons/SIR_A\",\n",
    "        help=\"the id of the gym environment\")\n",
    "#     parser.add_argument(\"--learning-rate\", type=float, default=3e-4,\n",
    "#         help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=700000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "#     parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "#         help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "#     parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "#         help=\"if toggled, cuda will be enabled by default\")\n",
    "#     parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "#         help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "#     parser.add_argument(\"--wandb-project-name\", type=str, default=\"ppo-implementation-details\",\n",
    "#         help=\"the wandb's project name\")\n",
    "#     parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "#         help=\"the entity (team) of wandb's project\")\n",
    "#     parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "#         help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
    "\n",
    "#     parser.add_argument(\"--policy_plot_interval\", type=int, default=1,\n",
    "#         help=\"seed of the experiment\")\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    parser.add_argument(\"--learning-starts\", type=int, default=5000,\n",
    "        help=\"learning starts\")\n",
    "    parser.add_argument(\"--target-entropy-scale\", type=int, default=1,\n",
    "        help=\"scale of target entropy with the dimension of action\")\n",
    "    parser.add_argument(\"--train-freq\", type=int, default=146,\n",
    "        help=\"train freq\")\n",
    "    parser.add_argument(\"--gradient-steps\", type=int, default=1,\n",
    "        help=\"gradient steps\")\n",
    "#     parser.add_argument(\"--num-envs\", type=int, default=1,\n",
    "#         help=\"the number of parallel game environments\")\n",
    "#     parser.add_argument(\"--num-steps\", type=int, default=2048,\n",
    "#         help=\"the number of steps to run in each environment per policy rollout\")\n",
    "#     parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "#         help=\"Toggle learning rate annealing for policy and value networks\")\n",
    "#     parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "#         help=\"Use GAE for advantage computation\")\n",
    "#     parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "#         help=\"the discount factor gamma\")\n",
    "#     parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
    "#         help=\"the lambda for the general advantage estimation\")\n",
    "#     parser.add_argument(\"--num-minibatches\", type=int, default=32,\n",
    "#         help=\"the number of mini-batches\")\n",
    "#     parser.add_argument(\"--update-epochs\", type=int, default=10,\n",
    "#         help=\"the K epochs to update the policy\")\n",
    "#     parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "#         help=\"Toggles advantages normalization\")\n",
    "#     parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n",
    "#         help=\"the surrogate clipping coefficient\")\n",
    "#     parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "#         help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
    "#     parser.add_argument(\"--ent-coef\", type=float, default=0.0,\n",
    "#         help=\"coefficient of the entropy\")\n",
    "#     parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
    "#         help=\"coefficient of the value function\")\n",
    "#     parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
    "#         help=\"the maximum norm for the gradient clipping\")\n",
    "#     parser.add_argument(\"--target-kl\", type=float, default=None,\n",
    "#         help=\"the target KL divergence threshold\")\n",
    "    if main_args is not None:\n",
    "        args = parser.parse_args(main_args.split())\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "#     args.num_steps //= PERIOD\n",
    "    args.total_timesteps //= PERIOD\n",
    "#     args.batch_size = int(args.num_envs * args.num_steps)\n",
    "#     args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82c583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with SIR_A__SAC__1__1733697534\n",
      "I am referencing the right package in the proj folder not venv\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to runs/SIR_A__SAC__1__1733697534_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IBM\\Projects\\rl\\rl-mai-paper-code\\RL-Epidemic-Benchmark\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 12       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 212      |\n",
      "---------------------------------\n",
      "At global step 292, total_rewards=-108584848.35782854\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -64.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 8        |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 451      |\n",
      "---------------------------------\n",
      "At global step 584, total_rewards=-107456652.75203751\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -53.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 690      |\n",
      "---------------------------------\n",
      "At global step 876, total_rewards=-109733855.16670915\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -48      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 8        |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 929      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -45.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 8        |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 1141     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.71    |\n",
      "|    critic_loss     | 0.193    |\n",
      "|    ent_coef        | 0.992    |\n",
      "|    ent_coef_loss   | -0.027   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28       |\n",
      "---------------------------------\n",
      "At global step 1168, total_rewards=-107542269.80721375\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -43.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 8        |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 1380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.978    |\n",
      "|    ent_coef_loss   | -0.0731  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 75       |\n",
      "---------------------------------\n",
      "At global step 1460, total_rewards=-100401529.9382114\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -41.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 1619     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.84    |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.964    |\n",
      "|    ent_coef_loss   | -0.122   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 123      |\n",
      "---------------------------------\n",
      "At global step 1752, total_rewards=-105398749.1744228\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -41.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 1858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 0.653    |\n",
      "|    ent_coef        | 0.95     |\n",
      "|    ent_coef_loss   | -0.169   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 171      |\n",
      "---------------------------------\n",
      "At global step 2044, total_rewards=-98928128.86522207\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -40.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 2097     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.94    |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.937    |\n",
      "|    ent_coef_loss   | -0.218   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -40      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 300      |\n",
      "|    total_timesteps | 2309     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 0.152    |\n",
      "|    ent_coef        | 0.925    |\n",
      "|    ent_coef_loss   | -0.257   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 261      |\n",
      "---------------------------------\n",
      "At global step 2336, total_rewards=-103286062.6977905\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -39.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 331      |\n",
      "|    total_timesteps | 2548     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 0.181    |\n",
      "|    ent_coef        | 0.912    |\n",
      "|    ent_coef_loss   | -0.307   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309      |\n",
      "---------------------------------\n",
      "At global step 2628, total_rewards=-97103802.58613834\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -39      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 2787     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.899    |\n",
      "|    ent_coef_loss   | -0.353   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 357      |\n",
      "---------------------------------\n",
      "At global step 2920, total_rewards=-95611613.16938417\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -38.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 3026     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 0.245    |\n",
      "|    ent_coef        | 0.886    |\n",
      "|    ent_coef_loss   | -0.403   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 405      |\n",
      "---------------------------------\n",
      "At global step 3212, total_rewards=-99554062.98087531\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -37.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 440      |\n",
      "|    total_timesteps | 3265     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.874    |\n",
      "|    ent_coef_loss   | -0.452   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 452      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -37.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 461      |\n",
      "|    total_timesteps | 3477     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 0.175    |\n",
      "|    ent_coef        | 0.863    |\n",
      "|    ent_coef_loss   | -0.492   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 495      |\n",
      "---------------------------------\n",
      "At global step 3504, total_rewards=-96965744.84996022\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -36.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 505      |\n",
      "|    total_timesteps | 3716     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.146    |\n",
      "|    ent_coef        | 0.85     |\n",
      "|    ent_coef_loss   | -0.539   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 543      |\n",
      "---------------------------------\n",
      "At global step 3796, total_rewards=-95852685.76746967\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -36.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 3955     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.145    |\n",
      "|    ent_coef        | 0.838    |\n",
      "|    ent_coef_loss   | -0.576   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 590      |\n",
      "---------------------------------\n",
      "At global step 4088, total_rewards=-96514247.97869958\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -36.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 581      |\n",
      "|    total_timesteps | 4194     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.827    |\n",
      "|    ent_coef_loss   | -0.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 638      |\n",
      "---------------------------------\n",
      "At global step 4380, total_rewards=-92926151.79805462\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -36.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 616      |\n",
      "|    total_timesteps | 4433     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.69    |\n",
      "|    critic_loss     | 0.114    |\n",
      "|    ent_coef        | 0.815    |\n",
      "|    ent_coef_loss   | -0.679   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 686      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -36.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 638      |\n",
      "|    total_timesteps | 4645     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.61    |\n",
      "|    critic_loss     | 0.131    |\n",
      "|    ent_coef        | 0.805    |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 728      |\n",
      "---------------------------------\n",
      "At global step 4672, total_rewards=-91591684.08197309\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -35.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 672      |\n",
      "|    total_timesteps | 4884     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.57    |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.793    |\n",
      "|    ent_coef_loss   | -0.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 776      |\n",
      "---------------------------------\n",
      "At global step 4964, total_rewards=-93458167.3907967\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -35.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 710      |\n",
      "|    total_timesteps | 5123     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.76    |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    ent_coef        | 0.782    |\n",
      "|    ent_coef_loss   | -0.809   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 824      |\n",
      "---------------------------------\n",
      "At global step 5256, total_rewards=-89783929.52675635\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -35.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 741      |\n",
      "|    total_timesteps | 5362     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.87    |\n",
      "|    critic_loss     | 0.105    |\n",
      "|    ent_coef        | 0.771    |\n",
      "|    ent_coef_loss   | -0.868   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 872      |\n",
      "---------------------------------\n",
      "At global step 5548, total_rewards=-85800114.05086632\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -35.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 775      |\n",
      "|    total_timesteps | 5601     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.83    |\n",
      "|    critic_loss     | 0.105    |\n",
      "|    ent_coef        | 0.76     |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 920      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -35      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 794      |\n",
      "|    total_timesteps | 5813     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.77    |\n",
      "|    critic_loss     | 40       |\n",
      "|    ent_coef        | 0.75     |\n",
      "|    ent_coef_loss   | -0.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 962      |\n",
      "---------------------------------\n",
      "At global step 5840, total_rewards=-88655238.17700586\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -32.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 826      |\n",
      "|    total_timesteps | 6052     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.82    |\n",
      "|    critic_loss     | 0.307    |\n",
      "|    ent_coef        | 0.74     |\n",
      "|    ent_coef_loss   | -0.985   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1010     |\n",
      "---------------------------------\n",
      "At global step 6132, total_rewards=-86263026.87559675\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -32.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 858      |\n",
      "|    total_timesteps | 6291     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3       |\n",
      "|    critic_loss     | 0.274    |\n",
      "|    ent_coef        | 0.729    |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1058     |\n",
      "---------------------------------\n",
      "At global step 6424, total_rewards=-84371480.49917509\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -32.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 894      |\n",
      "|    total_timesteps | 6530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.195    |\n",
      "|    ent_coef        | 0.719    |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1105     |\n",
      "---------------------------------\n",
      "At global step 6716, total_rewards=-84923739.49251674\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -32      |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 942      |\n",
      "|    total_timesteps | 6769     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.11    |\n",
      "|    critic_loss     | 0.173    |\n",
      "|    ent_coef        | 0.709    |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1153     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -31.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 960      |\n",
      "|    total_timesteps | 6981     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.09    |\n",
      "|    critic_loss     | 0.176    |\n",
      "|    ent_coef        | 0.7      |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1196     |\n",
      "---------------------------------\n",
      "At global step 7008, total_rewards=-84019768.14137742\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -31.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 991      |\n",
      "|    total_timesteps | 7220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.95    |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    ent_coef        | 0.69     |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1243     |\n",
      "---------------------------------\n",
      "At global step 7300, total_rewards=-87702482.60597701\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -31.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1021     |\n",
      "|    total_timesteps | 7459     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.19    |\n",
      "|    critic_loss     | 0.159    |\n",
      "|    ent_coef        | 0.68     |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1291     |\n",
      "---------------------------------\n",
      "At global step 7592, total_rewards=-80226918.14427647\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -31.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1053     |\n",
      "|    total_timesteps | 7698     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.11    |\n",
      "|    critic_loss     | 0.241    |\n",
      "|    ent_coef        | 0.671    |\n",
      "|    ent_coef_loss   | -1.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339     |\n",
      "---------------------------------\n",
      "At global step 7884, total_rewards=-80946789.32932337\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -31.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1085     |\n",
      "|    total_timesteps | 7937     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.28    |\n",
      "|    critic_loss     | 0.115    |\n",
      "|    ent_coef        | 0.661    |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1387     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1105     |\n",
      "|    total_timesteps | 8149     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.24    |\n",
      "|    critic_loss     | 0.142    |\n",
      "|    ent_coef        | 0.653    |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429     |\n",
      "---------------------------------\n",
      "At global step 8176, total_rewards=-80045617.6772575\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1137     |\n",
      "|    total_timesteps | 8388     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.92    |\n",
      "|    critic_loss     | 0.203    |\n",
      "|    ent_coef        | 0.644    |\n",
      "|    ent_coef_loss   | -1.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1477     |\n",
      "---------------------------------\n",
      "At global step 8468, total_rewards=-78179663.25788729\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1174     |\n",
      "|    total_timesteps | 8627     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.243    |\n",
      "|    ent_coef        | 0.635    |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1525     |\n",
      "---------------------------------\n",
      "At global step 8760, total_rewards=-78092784.24394126\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1209     |\n",
      "|    total_timesteps | 8866     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.11    |\n",
      "|    critic_loss     | 0.139    |\n",
      "|    ent_coef        | 0.626    |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1573     |\n",
      "---------------------------------\n",
      "At global step 9052, total_rewards=-78175992.85355121\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1242     |\n",
      "|    total_timesteps | 9105     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.23    |\n",
      "|    critic_loss     | 0.181    |\n",
      "|    ent_coef        | 0.617    |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1620     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1264     |\n",
      "|    total_timesteps | 9317     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.18    |\n",
      "|    critic_loss     | 42.2     |\n",
      "|    ent_coef        | 0.609    |\n",
      "|    ent_coef_loss   | -1.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1663     |\n",
      "---------------------------------\n",
      "At global step 9344, total_rewards=-78127137.58665247\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1300     |\n",
      "|    total_timesteps | 9556     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.91    |\n",
      "|    critic_loss     | 0.232    |\n",
      "|    ent_coef        | 0.601    |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1711     |\n",
      "---------------------------------\n",
      "At global step 9636, total_rewards=-73442644.09069131\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1339     |\n",
      "|    total_timesteps | 9795     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.33    |\n",
      "|    critic_loss     | 0.324    |\n",
      "|    ent_coef        | 0.592    |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1758     |\n",
      "---------------------------------\n",
      "At global step 9928, total_rewards=-74794837.1144779\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1384     |\n",
      "|    total_timesteps | 10034    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.19    |\n",
      "|    critic_loss     | 0.106    |\n",
      "|    ent_coef        | 0.584    |\n",
      "|    ent_coef_loss   | -1.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1806     |\n",
      "---------------------------------\n",
      "At global step 10220, total_rewards=-74222460.56650247\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1425     |\n",
      "|    total_timesteps | 10273    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.24    |\n",
      "|    critic_loss     | 0.133    |\n",
      "|    ent_coef        | 0.576    |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1854     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1452     |\n",
      "|    total_timesteps | 10485    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.157    |\n",
      "|    ent_coef        | 0.569    |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1896     |\n",
      "---------------------------------\n",
      "At global step 10512, total_rewards=-72742672.27222121\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -30      |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1498     |\n",
      "|    total_timesteps | 10724    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.25    |\n",
      "|    critic_loss     | 0.268    |\n",
      "|    ent_coef        | 0.561    |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1944     |\n",
      "---------------------------------\n",
      "At global step 10804, total_rewards=-73905931.96538825\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1546     |\n",
      "|    total_timesteps | 10963    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.21    |\n",
      "|    critic_loss     | 0.0777   |\n",
      "|    ent_coef        | 0.553    |\n",
      "|    ent_coef_loss   | -1.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1992     |\n",
      "---------------------------------\n",
      "At global step 11096, total_rewards=-71087687.47254334\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1587     |\n",
      "|    total_timesteps | 11202    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.42    |\n",
      "|    critic_loss     | 0.195    |\n",
      "|    ent_coef        | 0.545    |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2040     |\n",
      "---------------------------------\n",
      "At global step 11388, total_rewards=-73098886.38524182\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1621     |\n",
      "|    total_timesteps | 11441    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.29    |\n",
      "|    critic_loss     | 0.118    |\n",
      "|    ent_coef        | 0.537    |\n",
      "|    ent_coef_loss   | -2       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2088     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1643     |\n",
      "|    total_timesteps | 11653    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.17    |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    ent_coef        | 0.531    |\n",
      "|    ent_coef_loss   | -2       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2130     |\n",
      "---------------------------------\n",
      "At global step 11680, total_rewards=-67894101.63373324\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1675     |\n",
      "|    total_timesteps | 11892    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.13    |\n",
      "|    critic_loss     | 0.0907   |\n",
      "|    ent_coef        | 0.523    |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2178     |\n",
      "---------------------------------\n",
      "At global step 11972, total_rewards=-70150494.96774797\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1712     |\n",
      "|    total_timesteps | 12131    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.02    |\n",
      "|    critic_loss     | 0.102    |\n",
      "|    ent_coef        | 0.516    |\n",
      "|    ent_coef_loss   | -2.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2226     |\n",
      "---------------------------------\n",
      "At global step 12264, total_rewards=-71358346.47838926\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1746     |\n",
      "|    total_timesteps | 12370    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.16    |\n",
      "|    critic_loss     | 0.134    |\n",
      "|    ent_coef        | 0.509    |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2273     |\n",
      "---------------------------------\n",
      "At global step 12556, total_rewards=-67884632.69574809\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1785     |\n",
      "|    total_timesteps | 12609    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.21    |\n",
      "|    critic_loss     | 0.0819   |\n",
      "|    ent_coef        | 0.502    |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2321     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1807     |\n",
      "|    total_timesteps | 12821    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.16    |\n",
      "|    critic_loss     | 0.48     |\n",
      "|    ent_coef        | 0.495    |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2364     |\n",
      "---------------------------------\n",
      "At global step 12848, total_rewards=-70687307.24113184\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1842     |\n",
      "|    total_timesteps | 13060    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.28    |\n",
      "|    critic_loss     | 0.0579   |\n",
      "|    ent_coef        | 0.489    |\n",
      "|    ent_coef_loss   | -2.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2411     |\n",
      "---------------------------------\n",
      "At global step 13140, total_rewards=-68545434.4996221\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1877     |\n",
      "|    total_timesteps | 13299    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.23    |\n",
      "|    critic_loss     | 0.118    |\n",
      "|    ent_coef        | 0.482    |\n",
      "|    ent_coef_loss   | -2.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459     |\n",
      "---------------------------------\n",
      "At global step 13432, total_rewards=-67760109.26604122\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1914     |\n",
      "|    total_timesteps | 13538    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.78    |\n",
      "|    critic_loss     | 0.0647   |\n",
      "|    ent_coef        | 0.475    |\n",
      "|    ent_coef_loss   | -2.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2507     |\n",
      "---------------------------------\n",
      "At global step 13724, total_rewards=-66731621.83318305\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1958     |\n",
      "|    total_timesteps | 13777    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.91    |\n",
      "|    critic_loss     | 0.136    |\n",
      "|    ent_coef        | 0.468    |\n",
      "|    ent_coef_loss   | -2.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2555     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 1979     |\n",
      "|    total_timesteps | 13989    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.37    |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | -2.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2597     |\n",
      "---------------------------------\n",
      "At global step 14016, total_rewards=-69002415.38549769\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 7        |\n",
      "|    time_elapsed    | 2030     |\n",
      "|    total_timesteps | 14228    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.91    |\n",
      "|    critic_loss     | 0.183    |\n",
      "|    ent_coef        | 0.456    |\n",
      "|    ent_coef_loss   | -2.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2645     |\n",
      "---------------------------------\n",
      "At global step 14308, total_rewards=-67852436.09591483\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -29      |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 2073     |\n",
      "|    total_timesteps | 14467    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.16    |\n",
      "|    critic_loss     | 0.0898   |\n",
      "|    ent_coef        | 0.45     |\n",
      "|    ent_coef_loss   | -2.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2693     |\n",
      "---------------------------------\n",
      "At global step 14600, total_rewards=-66387609.094191335\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53       |\n",
      "|    ep_rew_mean     | -28.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 2113     |\n",
      "|    total_timesteps | 14706    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.11    |\n",
      "|    critic_loss     | 0.169    |\n",
      "|    ent_coef        | 0.444    |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2741     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(\"--gym-id SIR_A --seed 1 --learning-starts 1000 --train-freq 5\")\n",
    "run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "env = make_env(args.gym_id, args.seed, 0)()\n",
    "test_env = make_primal_env(args.gym_id)()\n",
    "\n",
    "import torch\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "\n",
    "class SummaryWriterCallback(BaseCallback):\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__()\n",
    "        self.best_total_r = -math.inf \n",
    "    def _on_training_start(self):\n",
    "        self._log_freq = 292  # log every 1000 calls\n",
    "\n",
    "        output_formats = self.logger.output_formats\n",
    "        # Save reference to tensorboard formatter object\n",
    "        # note: the failure case (not formatter found) is not handled here, should be done with try/except.\n",
    "        self.tb_formatter = next(formatter for formatter in output_formats if isinstance(formatter, TensorBoardOutputFormat))\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            env_obs = torch.Tensor(self.training_env.reset())\n",
    "            test_obs = torch.Tensor(test_env.reset())\n",
    "            done = False\n",
    "            timestep = 0\n",
    "            total_r = 0\n",
    "            itv_line = []\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action_mean, _ = self.model.predict(env_obs, deterministic=True)\n",
    "                    action_mean = np.array(action_mean)\n",
    "                    test_action_mean = np.clip(np.mean(action_mean, axis=0), 0, 1)\n",
    "                    \n",
    "                test_obs, r, done, _ = test_env.step(test_action_mean)\n",
    "                test_obs = torch.Tensor(test_obs)\n",
    "                itv_index = 0\n",
    "                itv_array = []\n",
    "                for itv in test_env.epi.static.interventions:\n",
    "                    if not itv.is_cost:\n",
    "                        v = float(test_action_mean[itv_index])\n",
    "                        self.tb_formatter.writer.add_scalar('charts/policy_{}/{}'.format(self.num_timesteps, itv.name), v, timestep)\n",
    "                        itv_array.append(v)\n",
    "                        itv_index += 1\n",
    "                itv_line.append(itv_array)\n",
    "                        \n",
    "                env_obs, _, _, _ = self.training_env.step(action_mean)\n",
    "                env_obs = torch.Tensor(env_obs)\n",
    "                \n",
    "                total_r += r\n",
    "                timestep += PERIOD\n",
    "            line = '|'.join([str(self.num_timesteps), str(total_r), str(itv_line)]) + '\\n'\n",
    "            self.tb_formatter.writer.add_scalar(\"charts/learning_curve\", total_r, self.num_timesteps)\n",
    "            print(\"At global step {}, total_rewards={}\".format(self.num_timesteps, total_r))\n",
    "            self.tb_formatter.writer.flush()\n",
    "            csv_file = open('runs/{}_1/records.csv'.format(run_name), 'a')\n",
    "            csv_file.write(line)\n",
    "            csv_file.close()\n",
    "            line2 = '|'.join([str(self.num_timesteps), str(env.obs_rms.mean), str(env.obs_rms.var), str(env.obs_rms.count)]) + '\\n' \n",
    "            csv_file2 = open('runs/{}_1/obs_normalization.csv'.format(run_name), 'a')\n",
    "            csv_file2.write(line2)\n",
    "            csv_file2.close()\n",
    "\n",
    "            \"\"\"\n",
    "            Saving Model Checkpoints \n",
    "            \"\"\"\n",
    "            if total_r > self.best_total_r: \n",
    "                self.best_total_r = total_r \n",
    "                print(\"Saving Best Checkpoint:\") \n",
    "                self.model.save('runs/{}_1/model_checkpoints/'.format(run_name) )\n",
    "        return True\n",
    "\n",
    "# learning_starts = [1000, 5000, 10000]\n",
    "# target_entropies = [-env.action_space.shape[0], -2*env.action_space.shape[0], -4*env.action_space.shape[0]]\n",
    "# choice = 0\n",
    "# learning_start = learning_starts[choice // 3]\n",
    "# target_entropy = target_entropies[choice % 3]\n",
    "\n",
    "print(f\"Running with {run_name}\")\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"runs/\", learning_starts=args.learning_starts, target_entropy=-args.target_entropy_scale * env.action_space.shape[0],\n",
    "           train_freq=args.train_freq, gradient_steps=args.gradient_steps)\n",
    "model.learn(total_timesteps=args.total_timesteps, log_interval=4, callback=SummaryWriterCallback(), tb_log_name=run_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
